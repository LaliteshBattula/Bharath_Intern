{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjpOV-i_UmCo",
        "outputId": "a6070df0-407b-49dc-831d-71e0e11a46e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1329atgUp97",
        "outputId": "b94c5418-9a7f-4726-bdc4-44fdb29ce7bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zDWwOz5UwbT",
        "outputId": "6a15fd0d-649b-4da2-86c3-f0a6161b9409"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7qqQfn1UzlG",
        "outputId": "ea795b67-67d3-4a0f-8c37-85652289e5ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3957 sha256=1558bb8ffb6470757985750f51c61203e5a7e64f340a84f37510c3ee57f6d25f\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import spacy\n",
        "import docx2txt\n",
        "import PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON_kJCtxU28-",
        "outputId": "b8cd8a1d-2ca4-48ff-a851-9234a181fec9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading English language model for spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "u2bEZ2RHU-_J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting name from the resume\n",
        "def extract_name(resume_text):\n",
        "    doc = nlp(resume_text)\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == 'PERSON':\n",
        "            return entity.text\n",
        "    return None"
      ],
      "metadata": {
        "id": "NClobtObVGMG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting projects from the resume\n",
        "def extract_projects(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)  # tokenize the resume text into sentences\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    projects = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)  # tokenize each sentence into words\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]  # remove stopwords and convert to lowercase\n",
        "\n",
        "        # checking for keywords related to projects\n",
        "        project_keywords = ['project', 'projects']\n",
        "        if any(word in filtered_words for word in project_keywords):\n",
        "            projects.append(sentence)\n",
        "\n",
        "    return projects"
      ],
      "metadata": {
        "id": "2t9pihuaVTVb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting certifications from the resume\n",
        "def extract_certifications(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    certifications = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # checking for keywords related to certifications\n",
        "        certification_keywords = ['certification', 'certifications']\n",
        "        if any(word in filtered_words for word in certification_keywords):\n",
        "            certifications.append(sentence)\n",
        "\n",
        "    return certifications"
      ],
      "metadata": {
        "id": "sF0TWul4WA38"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting skills from the resume\n",
        "def extract_skills(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    skills = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # checking for keywords related to experience\n",
        "        skill_keywords = ['skills']\n",
        "        if any(word in filtered_words for word in skill_keywords):\n",
        "            skills.append(sentence)\n",
        "\n",
        "    return skills"
      ],
      "metadata": {
        "id": "oH7UVztUW6Q1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting experience from the resume\n",
        "def extract_experience(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    experience = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # checking for keywords related to experience\n",
        "        experience_keywords = ['experience', 'worked', 'years']\n",
        "        if any(word in filtered_words for word in experience_keywords):\n",
        "            experience.append(sentence)\n",
        "\n",
        "    return experience"
      ],
      "metadata": {
        "id": "Jnkm2iShXit9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resume_parser(resume_text):\n",
        "    name = extract_name(resume_text)  # extracting name\n",
        "\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    skills = []\n",
        "    experience = []\n",
        "    projects = []\n",
        "    certifications = []\n",
        "\n",
        "    skills = extract_skills(resume_text)  # extracting skills\n",
        "    experience = extract_experience(resume_text)  # extracting experience\n",
        "    projects = extract_projects(resume_text)  # extracting projects\n",
        "    certifications = extract_certifications(resume_text)  #extracting certifications\n",
        "\n",
        "    return name, skills, experience, projects, certifications\n"
      ],
      "metadata": {
        "id": "4mp2lleYX_-n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt user to enter the resume file path\n",
        "resume_file = input(\"Enter the path to your resume file (DOCX or PDF): \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-AOX-UiYesj",
        "outputId": "92eeeeae-df5f-48c6-c3b3-896216b1e759"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to your resume file (DOCX or PDF): /content/Jane_Resume.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# determining the file extension\n",
        "file_extension = resume_file.split('.')[-1].lower()\n",
        "\n",
        "if file_extension == 'docx':\n",
        "    resume_text = docx2txt.process(resume_file)\n",
        "elif file_extension == 'pdf':\n",
        "    with open(resume_file, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        resume_text = ''\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            resume_text += page.extract_text()\n",
        "else:\n",
        "    print(\"Unsupported file format. Please provide a DOCX or PDF file.\")\n",
        "    exit()\n",
        "\n",
        "name, parsed_skills, parsed_experience, parsed_projects, parsed_certifications = resume_parser(resume_text)"
      ],
      "metadata": {
        "id": "w88r9QAFZ0CC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Name:\", name)\n",
        "print(\"Skills:\", parsed_skills)\n",
        "print(\"Experience:\", parsed_experience)\n",
        "print(\"Projects:\", parsed_projects)\n",
        "print(\"Certifications:\", parsed_certifications)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVnfjQ5WaZaX",
        "outputId": "0c3fe6ea-054b-4e29-8bb2-2123df9ecdcf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Jane Smith\n",
            "Skills: ['Education: \\nBachelor of Business Administration, University of XYZ \\n- Major: Marketing \\n- Relevant coursework: Digital Marketing, Consumer Behavior, Market Research \\nSkills: \\n- Social media management: Facebook, Instagram, Twi tter, LinkedIn \\n- Digital advertising: Google Ads, Facebook Ads, Di splay Ads \\n- Data analysis: Google Analytics, Excel, SQL \\n- Content creation: Canva, Adobe Photoshop \\nProjects: \\nEmail Marketing Automation \\n- Implemented an automated email marketing system t o streamline customer communication and \\nincrease conversion rates.']\n",
            "Experience: ['Experience: \\nDigital Marketing Specialist, XYZ Agency \\n- Developed and executed digital marketing campaign s across various platforms including social \\nmedia, email, and display advertising.']\n",
            "Projects: ['Education: \\nBachelor of Business Administration, University of XYZ \\n- Major: Marketing \\n- Relevant coursework: Digital Marketing, Consumer Behavior, Market Research \\nSkills: \\n- Social media management: Facebook, Instagram, Twi tter, LinkedIn \\n- Digital advertising: Google Ads, Facebook Ads, Di splay Ads \\n- Data analysis: Google Analytics, Excel, SQL \\n- Content creation: Canva, Adobe Photoshop \\nProjects: \\nEmail Marketing Automation \\n- Implemented an automated email marketing system t o streamline customer communication and \\nincrease conversion rates.']\n",
            "Certifications: ['Certifications: \\n- Google Ads Certification \\n- HubSpot Inbound Marketing Certification']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import spacy\n",
        "import docx2txt\n",
        "import PyPDF2\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    doc = nlp(resume_text)\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == 'PERSON':\n",
        "            return entity.text\n",
        "    return None\n",
        "\n",
        "def extract_projects(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)  # tokenize the resume text into sentences\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    projects = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)  # tokenize each sentence into words\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]  # remove stopwords and convert to lowercase\n",
        "\n",
        "        # checking for keywords related to projects\n",
        "        project_keywords = ['project', 'projects']\n",
        "        if any(word in filtered_words for word in project_keywords):\n",
        "            projects.append(sentence)\n",
        "\n",
        "    return projects\n",
        "\n",
        "def extract_certifications(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    certifications = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # checking for keywords related to certifications\n",
        "        certification_keywords = ['certification', 'certifications']\n",
        "        if any(word in filtered_words for word in certification_keywords):\n",
        "            certifications.append(sentence)\n",
        "\n",
        "    return certifications\n",
        "\n",
        "def extract_skills(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    skills = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # checking for keywords related to experience\n",
        "        skill_keywords = ['skills']\n",
        "        if any(word in filtered_words for word in skill_keywords):\n",
        "            skills.append(sentence)\n",
        "\n",
        "    return skills\n",
        "\n",
        "def extract_experience(resume_text):\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    experience = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # checking for keywords related to experience\n",
        "        experience_keywords = ['experience', 'worked', 'years']\n",
        "        if any(word in filtered_words for word in experience_keywords):\n",
        "            experience.append(sentence)\n",
        "\n",
        "    return experience\n",
        "\n",
        "def resume_parser(resume_text):\n",
        "    name = extract_name(resume_text)  # extracting name\n",
        "\n",
        "    sentences = sent_tokenize(resume_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    skills = []\n",
        "    experience = []\n",
        "    projects = []\n",
        "    certifications = []\n",
        "\n",
        "    skills = extract_skills(resume_text)  # extracting skills\n",
        "    experience = extract_experience(resume_text)  # extracting experience\n",
        "    projects = extract_projects(resume_text)  # extracting projects\n",
        "    certifications = extract_certifications(resume_text)  #extracting certifications\n",
        "\n",
        "    return name, skills, experience, projects, certifications\n",
        "\n",
        "resume_file = input(\"Enter the path to your resume file (DOCX or PDF): \")\n",
        "file_extension = resume_file.split('.')[-1].lower()\n",
        "\n",
        "if file_extension == 'docx':\n",
        "    resume_text = docx2txt.process(resume_file)\n",
        "elif file_extension == 'pdf':\n",
        "    with open(resume_file, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        resume_text = ''\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            resume_text += page.extract_text()\n",
        "else:\n",
        "    print(\"Unsupported file format. Please provide a DOCX or PDF file.\")\n",
        "    exit()\n",
        "\n",
        "name, parsed_skills, parsed_experience, parsed_projects, parsed_certifications = resume_parser(resume_text)\n",
        "\n",
        "print(\"Name:\", name)\n",
        "print(\"Skills:\", parsed_skills)\n",
        "print(\"Experience:\", parsed_experience)\n",
        "print(\"Projects:\", parsed_projects)\n",
        "print(\"Certifications:\", parsed_certifications)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWZl4-9EgH6H",
        "outputId": "fb790614-9382-46a1-f886-7eb0bad369bc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to your resume file (DOCX or PDF): /content/Jane_Resume.docx\n",
            "Name: Jane Smith\n",
            "Skills: ['Education:\\n\\nBachelor of Business Administration, University of XYZ\\n\\n- Major: Marketing\\n\\n- Relevant coursework: Digital Marketing, Consumer Behavior, Market Research\\n\\nSkills:\\n\\n- Social media management: Facebook, Instagram, Twitter, LinkedIn\\n\\n- Digital advertising: Google Ads, Facebook Ads, Display Ads\\n\\n- Data analysis: Google Analytics, Excel, SQL\\n\\n- Content creation: Canva, Adobe Photoshop\\n\\nProjects:\\n\\nEmail Marketing Automation\\n\\n- Implemented an automated email marketing system to streamline customer communication and increase conversion rates.']\n",
            "Experience: ['Experience:\\n\\nDigital Marketing Specialist, XYZ Agency\\n\\n- Developed and executed digital marketing campaigns across various platforms including social media, email, and display advertising.']\n",
            "Projects: ['Education:\\n\\nBachelor of Business Administration, University of XYZ\\n\\n- Major: Marketing\\n\\n- Relevant coursework: Digital Marketing, Consumer Behavior, Market Research\\n\\nSkills:\\n\\n- Social media management: Facebook, Instagram, Twitter, LinkedIn\\n\\n- Digital advertising: Google Ads, Facebook Ads, Display Ads\\n\\n- Data analysis: Google Analytics, Excel, SQL\\n\\n- Content creation: Canva, Adobe Photoshop\\n\\nProjects:\\n\\nEmail Marketing Automation\\n\\n- Implemented an automated email marketing system to streamline customer communication and increase conversion rates.']\n",
            "Certifications: ['Certifications:\\n\\n- Google Ads Certification\\n\\n- HubSpot Inbound Marketing Certification']\n"
          ]
        }
      ]
    }
  ]
}